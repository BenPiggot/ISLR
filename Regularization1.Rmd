###Best Subset Model Selection

First, I am going to explore best subset model selection using the Hitters data from the ISLR library.

```{r}
library(ISLR)
data(Hitters)
summary(Hitters)
```
There are some missing values, so we will remove them before we proceed.

```{r}
Hitters <- na.omit(Hitters)
with(Hitters, sum(is.na(Salary)))
```
Best subset regression, using the package "leaps," which allows us to perform best subset regression somewhat painlessly.
```{r}
library(leaps)
regfit.full <- regsubsets(Salary~., data=Hitters)
summary(regfit.full)
```
The above gives best subsets output for 8 variables; lets now specify that we want a best subsets model that measures combinations of all 19 variables in the Hitters data set.
```{r}
regfit.full <- regsubsets(Salary~., data=Hitters, nvmax=19)
reg.summary <- summary(regfit.full)
names(reg.summary)
plot(reg.summary$cp,xlab="Number of Variables", ylab="Cp", type="b")
which.min(reg.summary$cp)
```
There is also a plot method for the 'regsubsets' object as well:
```{r}
plot(regfit.full, scale="Cp")
coef(regfit.full, 10)
```
  
### Forward Stepwise Selection:

Here, I can continue to use regsubsets but specify my method as 'forward':
```{r}
regfit.fwd <- regsubsets(Salary~., data=Hitters, nvmax=19, method="forward")
regfwd.summary <- summary(regfit.fwd)
plot(regfit.fwd, scale="Cp")
plot(regfwd.summary$cp,xlab="Number of Variables", ylab="Cp", type="b")
```
  
### Model Selection Using a Validation Set

Now I will create a training and a validation data set so that I can choose a good subset model.
```{r}
dim(Hitters)
set.seed(143)
train <- sample(seq(263), 180, replace=FALSE)
train
regfit.fwd <- regsubsets(Salary~., data=Hitters[train,], nvmax=19, 
                         method="forward")
```
Now, I will make a prediction on the data from Hitters not in the training data set (e.g. the validation data set):
```{r}
val.errors <- rep(NA,19)
x.test <- model.matrix(Salary~., data=Hitters[-train,])
for(i in 1:19) {
    coefi <- coef(regfit.fwd, id=i)
    pred <- x.test[,names(coefi)]%*%coefi
    val.errors[i] <- mean((Hitters$Salary[-train] - pred)^2)
}

plot(sqrt(val.errors), ylab="Root MSE", xlab="Number of Variables", 
     type="b", ylim=c(250,400), pch=19)
points(sqrt(regfit.fwd$rss[-1]/180),col="blue", pch=19, type="b")
legend("topright", legend=c("training","validation"),
       col=c("blue","black"),pch=19,cex=.85)
```
Because it was a little bit time consuming not having a a predict function for 'regsubsets', I will now write one:
```{r}
predict.regsubsets <- function(object,newdata,id,...){
    form <- as.formula(object$call[[2]])
    mat <- model.matrix(form,newdata)
    coefi <- coef(object, id=id)
    pred <- mat[,names(coefi)]%*%coefi
}
```

### Model Selection by Cross-Validation
With the above formula, I will now do 10-fold cross-validation:
```{r}
set.seed(44)
folds <- sample(rep(1:10, length=nrow(Hitters)))
table(folds)
cv.errors <- matrix(NA,10,19)
for(k in 1:10){
    best.fit <- regsubsets(Salary ~ ., data=Hitters[folds!=k,],
                           nvmax=19, method="forward")
    for(i in 1:19){
        pred <- predict.regsubsets(best.fit, Hitters[folds==k,],id=i)
        cv.errors[k,i] <- mean((Hitters$Salary[folds==k] - pred)^2)
    }
}
rmse.cv <- sqrt(apply(cv.errors,2,mean))
plot(rmse.cv, pch=19, type="b")
```

### Ridge Regression and the Lasso
I will now use the 'glmnet' which does not use model formula language, so I will need to create an 'x' and 'y':
```{r}
library(glmnet)
x <- model.matrix(Salary ~. -1, data=Hitters)
y <- Hitters$Salary
```
I will fit a ridge regression by calling 'glmnet' with 'alpha=0' (See helpfile - 'alpha=0' fits a ridge regression, while 'alpha=1' fits a lass model, while alphas in-between 0 and 1 fit 'elastic net models'):
```{r}
fit.ridge <- glmnet(x,y,alpha=0)
plot(fit.ridge, xvar="lambda", label=TRUE)
cv.ridge <- cv.glmnet(x,y,alpha=0)
plot(cv.ridge)
```
Now I will fit a lasso model and examine its output in various ways:
```{r}
fit.lasso <- glmnet(x,y,alpha=1)
plot(fit.lasso, xvar="lambda", label=TRUE)
cv.lasso <- cv.glmnet(x,y,alpha=1)
plot(cv.lasso)
plot(fit.lasso, xvar="dev", label=TRUE)
coef(cv.lasso)
```
How about using a trial/validation model to select the lambda for the lasso?

```{r}
lasso.tr <- glmnet(x[train,], y[train])
lasso.tr
pred <- predict(lasso.tr, x[-train,])
dim(pred)
rmse <- sqrt(apply((y[-train]-pred)^2, 2, mean))
plot(log(lasso.tr$lambda),rmse, type="b", xlab="log lambda")
lam.best <- lasso.tr$lambda[order(rmse)[1]]
coef(lasso.tr, s=lam.best)
```

### PCR and PLS Regression
Finally, I will fit models using principal components regression and partial least squares methods. First, I will fit a PCR model with standardized predictors (scale=TRUE) and using 10-fold cross-validation (validation="CV") and plot its output:
```{r}
library(pls)
set.seed(2)
pcr.fit <- pcr(Salary ~., data=Hitters, scale=TRUE, 
               validation="CV")
summary(pcr.fit)
plot(pcr.fit)
validationplot(pcr.fit)
```
Below, I fit the same PCR model on my training data and use the resulting fit on my testing data. I evaluate the model's predictive value by measuring root MSE of the prediction: 
```{r}
pcr.fit <- pcr(Salary ~., data=Hitters, subset=train, scale=TRUE, 
               validation="CV")
summary(pcr.fit)
validationplot(pcr.fit)
pcr.pred <- predict(pcr.fit, Hitters[-train,],ncomp=16)
summary(pcr.pred)
sqrt(mean((Hitters$Salary[-train] - pcr.pred)^2))
```
Finally, I fit a partial least model with standardized predictors (scale=TRUE) and using 10-fold cross-validation (validation="CV") and plot its output:
```{r}
set.seed(312)
pls.fit <- plsr(Salary ~., data=Hitters, subset=train, 
                scale=TRUE, validation="CV")
summary(pls.fit)
validationplot(pls.fit)
```
I then evaluate the corresponding test set RMSE:
```{r}
pls.pred <- predict(pls.fit, Hitters[-train,],ncomp=3)
summary(pls.pred)
mean((pls.pred - Hitters$Salary[-train])^2)
```

