
Nonlinear Models
=========================================

Here, I explore nonlinear models using some tools in R:
```{r}
library(ISLR)
data(Wage)
```

###Polynomials
First, I will use polynomials and focus on a single variable, Age:

```{r}
fit <- lm(wage ~ poly(age, 4), data=Wage)
summary(fit)
```
Let's now make a plot of the fitted function, along with the standard error of the fit.  
```{r fig.width=7, fig.height=6}
agelims <- range(Wage$age)
age.grid <- seq(from=agelims[1], to=agelims[2])
preds <- predict(fit, newdata=list(age=age.grid), se=TRUE)
se.bands <- cbind(preds$fit + 2*preds$se, preds$fit - 2*preds$se)
plot(Wage$age,Wage$wage,col="darkgrey",xlab="Age",ylab="Wage")
lines(age.grid,preds$fit,lwd=2,col="blue")
matlines(age.grid,se.bands,col="blue",lty=2)
```

There are other more direct ways of doing this in R. For example:
```{r}
fita <- lm(wage ~ I(age) + I(age^2) + I(age^3) + I(age^4), data=Wage)
summary(fita)
```
Note that the coefficients and p-values are different. However, the fits are the same, as the plot below reveals:
```{r}
plot(fitted(fit),fitted(fita))
```

By using orthogonal polynomials in this simple way, it turns on we can separately test for each coefficient. So if we look at the summary again, we can see that the linear, quadratic, and cubic terms are significant, but not the quartic.

However, using orthogonal polynomials in this way only works if one is using a linear model and if there is a single predictor. In these cases, we would use anova() to test the relative strength of different models:
```{r}
fita <- lm(wage ~ education, data=Wage)
fitb <- lm(wage ~ education + age, data=Wage)
fitc <- lm(wage ~ education + poly(age, 2), data=Wage)
fitd <- lm(wage ~ education + poly(age, 3), data=Wage)
anova(fita, fitb, fitc, fitd)
```

###Polynomial Logistic Regression
Now I fit a logistic regression model to a binary response variable constructed from 'wage'. We code the big earners ('>250K') as 1, else 0.
```{r}
fit <- glm(I(wage>250) ~ poly(age,3),data=Wage, family=binomial)
summary(fit)
preds <- predict(fit, list(age=age.grid), se=TRUE)
se.bands <- preds$fit + cbind(fit=0,lower=-2*preds$se, upper=2*preds$se)
se.bands[1:5,]
```
The computations above have been done on the logit scale. To transform them into probabilities, I need to apply the inverse logit mapping.
$$p=\frac{e^\eta}{1+e^\eta}.$$
(The inverse logit is formatted in TeX, which MarkDown can interpret.)

We can apply the inverse logit transormation for all three columns of the se.bands matrix simultaneously:
```{r}
prob.bands <- exp(se.bands)/(1+exp(se.bands))
matplot(age.grid, prob.bands, type="l", col="blue", 
        lwd=c(2,1,1),lty=c(1,2,2), ylim=c(0,.1))
points(jitter(Wage$age), I(Wage$wage>250)/10)
```

###Splines
Splines are more flexible than polynomials, but the idea for both is similar. Below, I explore cubic splines:

```{r}
library(splines)
fit <- lm(wage ~ bs(age, knots=c(25,40,50)), data=Wage)
summary(fit)
plot(Wage$age, Wage$wage, col="dark gray")
lines(age.grid, predict(fit, list(age=age.grid)), col="dark green", lwd=2)
abline(v=c(25,40,60), lty=2, col="dark green")
```

Smoothing splines do not require knot selection, but they do have a smoothing parameter, which can conveniently be specified via the effective degrees of freedom or 'df'.

```{r}
fit <- smooth.spline(Wage$age, Wage$wage, df=16)
lines(fit, col="red", lwd=2)

```

Rather than select an effective degrees of freedom, one can also use LOOCV to select the smoothing parameter:

```{r}
fit <- smooth.spline(Wage$age, Wage$wage, cv=TRUE)
lines(fit, col="purple", lwd=2)
fit$df
```

###Generalized Addititive Models (GAMs)

So far I have focused on fitting models with mostly single nonlinear terms. The 'gam' packages makes it easier to work with multiple nonlinear terms. In addition, it knows how to plot these functions and their standard errors.

```{r}
library(gam)
gam1 <- gam(wage ~ s(age, df=4) + s(year, df=4) + education, data=Wage)
par(mfrow = c(1,3))
plot(gam1, se=T)

gam2 <- gam(I(wage>250) ~ s(age, df=4) + s(year, df=4) + education, data=Wage)
plot(gam2)

gam2a <- gam(I(wage>250) ~ s(age, df=4) + year + education, data=Wage)
plot(gam2a)
anova(gam2a, gam2, test="Chisq")
```

One nice feature of the 'gam' package is that knows how to plot functions nicely, even for models fit by 'lm' and 'glm':

```{r fig.width=10, fig.height=5}
par(mfrow=c(1,3))
lm1 <- lm(wage ~ ns(age, df=4) + ns(year, df=4) + education, data=Wage)
plot.gam(lm1, se=T)
```

